# 🌟 **MyTransformers** 🌟

**项目概述**  
该项目的目标是利用个人数据（如日记和微信聊天记录）训练一个自定义的 GPT 模型 🤖📓。模型采用纯解码器的 Transformer 架构 📐，旨在探索使用私人数据预训练语言模型的效果 🌍，尽管此举在学术或商业上没有明显的价值，但对于个人成长具有极大的意义 💪✨。

---

## 🥰**Get Started**🥰

### Environment Setup
```bash
git clone https://github.com/NytePlus/MyTransformers.git
conda env create -n MyTransformers python=3.8
pip install d2l torch
```
### Start Chat

```bash
python main.py
```

---

## 🏗 **架构设计** 🏗
1. **上下文长度**：128 📝  
2. **模型类型**：Decoder Only 🛠️

---

## 📊 **数据** 📊
1. **预训练数据**：个人日记 🗒️  
2. **SFT（监督微调）数据**：微信聊天记录 💬

---

## 📅 **日志** 📅
这是一个记录打卡贴，作者计划优化一个映射函数 \( f: X \mapsto Y \) 🔄，逐步优化模型，分享过程和心得 💡。

项目仓库链接：  
🔗 [MyTransformers 项目仓库](https://github.com/NytePlus/MyTransformers)

---

## 🧐 **项目价值** 🧐
⚠️ **免责声明**：本项目没有学术或商业价值，主要是个人成长的探索 🚶‍♂️📈。  
本项目的重点是使用个人数据进行模型预训练，尽管现有的 Lora 微调或 prompt 现有大模型的方式更为高效 ⚙️，但使用独特的私人数据训练模型的好奇心驱动了这个项目 🧠🔍。如果这个探索能引起其他感兴趣的读者共鸣，那它就具备了意义 🎯💥。

---

## 📚 **关于深度学习的启发** 📚
lz来自工科大平台，大一上的时候在C++课上 🖥️和老师讨论快速排序的参考值选择的问题。讨论的结果是老师告诉我，你学这些算法没用 🧠，这些都是上个世纪的东西了，建议你看看《机器学习》 📖。于是lz看了看周志华的《机器学习》 📕，忽然觉得这正是我想找的东西 💡。

---

## ✍️ **关于日记的点滴** ✍️
小学四年级的时候，我开始每天记日记 📓。那时候好像每天都很开心，有各种各样有趣的事，于是日记里全是叙事 🕰。后来我还是一直记日记，初中、高中、大学，我能分明地感觉到自己的变化 💬，仿佛再也没有真正的开心过了😌➡️🤔，日记的风格从叙事变成大片大片的论证 ✍️📖。

---

## 🔥 **分享方向** 🔥
未来，作者计划从以下几个方面进行分享 📤：
- **原料（数据）** 🗃️：展示用于模型训练的日记和聊天记录 📂
- **丹方（架构设计）** 🛠️：深入解析神经网络的架构设计 🧩
- **代码与讲解** 💻：分享代码实现与详细解释 💡
- **训练过程** 🏋️：展示训练进展，并提供阶段性的在线 demo 供读者测试 🎮

---


💡 **项目的意义在于探索与坚持** 💡  
尽管重新做模型预训练在技术上可能不如微调现有模型高效 📉，但这段旅程更多的是对个人成长和技术好奇心的挑战 🧗‍♂️。希望能够在此过程中收获知识 🧠📈，也希望能与其他爱好者共鸣！💬

🤝 **如果你也对使用个人数据训练模型感兴趣，欢迎交流！** 🤗
